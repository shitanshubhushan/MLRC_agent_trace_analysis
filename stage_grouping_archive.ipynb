{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GROUP BY STEPS TAKEN TO FIX A ERROR\n",
    "#### PASS TO O3 AND GET BETTER GROUPING LIKE DEBUGGING\n",
    "Maybe add how many steps it took to fix the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_agent_file(folder_path):\n",
    "    # Find all agent files in the folder\n",
    "    agent_files = glob.glob(os.path.join(folder_path, \"agent_*_*.json\"))\n",
    "    \n",
    "    if not agent_files:\n",
    "        return None\n",
    "    \n",
    "    # Extract agent numbers and find the highest one\n",
    "    max_agent_num = 0\n",
    "    max_agent_file = None\n",
    "    \n",
    "    for file in agent_files:\n",
    "        # Extract the number from filename using regex\n",
    "        match = re.search(r\"agent_(\\d+)_\\d+\\.json\", file)\n",
    "        if match:\n",
    "            agent_num = int(match.group(1))\n",
    "            if agent_num > max_agent_num:\n",
    "                max_agent_num = agent_num\n",
    "                max_agent_file = file\n",
    "    \n",
    "    return max_agent_file\n",
    "\n",
    "def extract_history_steps(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        # Extract history_steps without observation field\n",
    "        history_steps = data.get('history_steps', [])\n",
    "        \n",
    "        # Remove observation field from each step\n",
    "        for step in history_steps:\n",
    "            if 'observation' in step:\n",
    "                del step['observation']\n",
    "\n",
    "            if 'action' in step and isinstance(step['action'], dict):\n",
    "                if 'Research Plan and Status' in step['action']:\n",
    "                    del step['action']['Research Plan and Status']\n",
    "                if 'Fact Check' in step['action']:\n",
    "                    del step['action']['Fact Check']\n",
    "                \n",
    "        return history_steps\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_task_name_and_id(folder_path):\n",
    "    # Split the path into components\n",
    "    parts = folder_path.split(os.sep)\n",
    "    \n",
    "    # Look for task name and run ID in the path\n",
    "    task_name = None\n",
    "    run_id = None\n",
    "    \n",
    "    for i, part in enumerate(parts):\n",
    "        if i < len(parts) - 2 and parts[i+2].startswith('0'):  # Assuming run IDs start with numbers\n",
    "            task_name = part\n",
    "            run_id = parts[i+2]\n",
    "            break\n",
    "    \n",
    "    return task_name, run_id\n",
    "\n",
    "def process_all_folders(base_path, output_base):\n",
    "    # Walk through directory structure\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        # Check if this folder contains agent files\n",
    "        agent_files = [f for f in files if f.startswith(\"agent_\") and f.endswith(\".json\")]\n",
    "        if agent_files:\n",
    "            last_agent = get_last_agent_file(root)\n",
    "            if last_agent:\n",
    "                history = extract_history_steps(last_agent)\n",
    "                if history:\n",
    "                    # Extract task name and run ID from the folder path\n",
    "                    task_name, run_id = extract_task_name_and_id(root)\n",
    "                    \n",
    "                    if task_name and run_id:\n",
    "                        # Create output directory\n",
    "                        output_dir = os.path.join(output_base, task_name, run_id)\n",
    "                        os.makedirs(output_dir, exist_ok=True)\n",
    "                        \n",
    "                        # Save the extracted history\n",
    "                        output_file = os.path.join(output_dir, \"output.json\")\n",
    "                        with open(output_file, 'w') as f:\n",
    "                            json.dump(history, f, indent=2)\n",
    "                        \n",
    "                        print(f\"Extracted history from {root} saved to {output_file}\")\n",
    "                    else:\n",
    "                        print(f\"Could not determine task name and run ID for {root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted history from data/machine_unlearning/claude-3-5-sonnet-v2/0203205627_279760/agent_log saved to Steps/machine_unlearning/0203205627_279760/output.json\n",
      "Extracted history from data/machine_unlearning/claude-3-5-sonnet-v2/0203205627_2107738/agent_log saved to Steps/machine_unlearning/0203205627_2107738/output.json\n",
      "Extracted history from data/machine_unlearning/claude-3-5-sonnet-v2/0204004906_2156777/agent_log saved to Steps/machine_unlearning/0204004906_2156777/output.json\n",
      "Extracted history from data/machine_unlearning/claude-3-5-sonnet-v2/0204021048_1005508/agent_log saved to Steps/machine_unlearning/0204021048_1005508/output.json\n",
      "Extracted history from data/machine_unlearning/claude-3-5-sonnet-v2/0204000906_980483/agent_log saved to Steps/machine_unlearning/0204000906_980483/output.json\n",
      "Extracted history from data/machine_unlearning/claude-3-5-sonnet-v2/0204002434_497948/agent_log saved to Steps/machine_unlearning/0204002434_497948/output.json\n",
      "Extracted history from data/machine_unlearning/claude-3-5-sonnet-v2/0203235437_311468/agent_log saved to Steps/machine_unlearning/0203235437_311468/output.json\n",
      "Extracted history from data/machine_unlearning/claude-3-5-sonnet-v2/0203205627_931197/agent_log saved to Steps/machine_unlearning/0203205627_931197/output.json\n",
      "Extracted history from data/meta-learning/claude-3-5-sonnet-v2/0211164506_234282/agent_log saved to Steps/meta-learning/0211164506_234282/output.json\n",
      "Extracted history from data/meta-learning/claude-3-5-sonnet-v2/0212001115_3514169/agent_log saved to Steps/meta-learning/0212001115_3514169/output.json\n",
      "Extracted history from data/meta-learning/claude-3-5-sonnet-v2/0212223706_984095/agent_log saved to Steps/meta-learning/0212223706_984095/output.json\n",
      "Extracted history from data/meta-learning/claude-3-5-sonnet-v2/0212230702_2432398/agent_log saved to Steps/meta-learning/0212230702_2432398/output.json\n",
      "Extracted history from data/meta-learning/claude-3-5-sonnet-v2/0212021042_3840295/agent_log saved to Steps/meta-learning/0212021042_3840295/output.json\n",
      "Extracted history from data/meta-learning/claude-3-5-sonnet-v2/0211142216_3241844/agent_log saved to Steps/meta-learning/0211142216_3241844/output.json\n",
      "Extracted history from data/meta-learning/claude-3-5-sonnet-v2/0212235435_1003034/agent_log saved to Steps/meta-learning/0212235435_1003034/output.json\n",
      "Extracted history from data/meta-learning/claude-3-5-sonnet-v2/0211192713_3421734/agent_log saved to Steps/meta-learning/0211192713_3421734/output.json\n",
      "Extracted history from data/llm-merging/claude-3-5-sonnet-v2/0121081654/agent_log saved to Steps/llm-merging/0121081654/output.json\n",
      "Extracted history from data/llm-merging/claude-3-5-sonnet-v2/0124071448/agent_log saved to Steps/llm-merging/0124071448/output.json\n",
      "Extracted history from data/llm-merging/claude-3-5-sonnet-v2/0124110854/agent_log saved to Steps/llm-merging/0124110854/output.json\n",
      "Extracted history from data/llm-merging/claude-3-5-sonnet-v2/0122121253/agent_log saved to Steps/llm-merging/0122121253/output.json\n",
      "Extracted history from data/llm-merging/claude-3-5-sonnet-v2/0122132158/agent_log saved to Steps/llm-merging/0122132158/output.json\n",
      "Extracted history from data/llm-merging/claude-3-5-sonnet-v2/0121200118/agent_log saved to Steps/llm-merging/0121200118/output.json\n",
      "Extracted history from data/llm-merging/claude-3-5-sonnet-v2/0121073211/agent_log saved to Steps/llm-merging/0121073211/output.json\n",
      "Extracted history from data/llm-merging/claude-3-5-sonnet-v2/0121214229/agent_log saved to Steps/llm-merging/0121214229/output.json\n",
      "Extracted history from data/backdoor-trigger-recovery/claude-3-5-sonnet-v2/0124032241/agent_log saved to Steps/backdoor-trigger-recovery/0124032241/output.json\n",
      "Extracted history from data/backdoor-trigger-recovery/claude-3-5-sonnet-v2/0121200721/agent_log saved to Steps/backdoor-trigger-recovery/0121200721/output.json\n",
      "Extracted history from data/backdoor-trigger-recovery/claude-3-5-sonnet-v2/0124144709/agent_log saved to Steps/backdoor-trigger-recovery/0124144709/output.json\n",
      "Extracted history from data/backdoor-trigger-recovery/claude-3-5-sonnet-v2/0122092147/agent_log saved to Steps/backdoor-trigger-recovery/0122092147/output.json\n",
      "Extracted history from data/backdoor-trigger-recovery/claude-3-5-sonnet-v2/0121045246/agent_log saved to Steps/backdoor-trigger-recovery/0121045246/output.json\n",
      "Extracted history from data/backdoor-trigger-recovery/claude-3-5-sonnet-v2/0122164524/agent_log saved to Steps/backdoor-trigger-recovery/0122164524/output.json\n",
      "Extracted history from data/backdoor-trigger-recovery/claude-3-5-sonnet-v2/0121105812/agent_log saved to Steps/backdoor-trigger-recovery/0121105812/output.json\n",
      "Extracted history from data/backdoor-trigger-recovery/claude-3-5-sonnet-v2/0122014648/agent_log saved to Steps/backdoor-trigger-recovery/0122014648/output.json\n",
      "Extracted history from data/perception_temporal_action_loc/claude-3-5-sonnet-v2/0130064726/agent_log saved to Steps/perception_temporal_action_loc/0130064726/output.json\n",
      "Extracted history from data/perception_temporal_action_loc/claude-3-5-sonnet-v2/0130104224/agent_log saved to Steps/perception_temporal_action_loc/0130104224/output.json\n",
      "Extracted history from data/perception_temporal_action_loc/claude-3-5-sonnet-v2/0130021740/agent_log saved to Steps/perception_temporal_action_loc/0130021740/output.json\n",
      "Extracted history from data/perception_temporal_action_loc/claude-3-5-sonnet-v2/0130045001/agent_log saved to Steps/perception_temporal_action_loc/0130045001/output.json\n",
      "Extracted history from data/perception_temporal_action_loc/claude-3-5-sonnet-v2/0130093111/agent_log saved to Steps/perception_temporal_action_loc/0130093111/output.json\n",
      "Extracted history from data/perception_temporal_action_loc/claude-3-5-sonnet-v2/0130112112/agent_log saved to Steps/perception_temporal_action_loc/0130112112/output.json\n",
      "Extracted history from data/perception_temporal_action_loc/claude-3-5-sonnet-v2/0130102918/agent_log saved to Steps/perception_temporal_action_loc/0130102918/output.json\n",
      "Extracted history from data/perception_temporal_action_loc/claude-3-5-sonnet-v2/0130102836/agent_log saved to Steps/perception_temporal_action_loc/0130102836/output.json\n"
     ]
    }
   ],
   "source": [
    "base_directory = \"data\"\n",
    "output_directory = \"Steps\"\n",
    "\n",
    "# Create the base output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Process all folders\n",
    "process_all_folders(base_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_analyze_files(steps_dir):\n",
    "    \n",
    "    class Step(BaseModel):\n",
    "        Step_ID: int\n",
    "        Stage: int\n",
    "\n",
    "    class MathReasoning(BaseModel):\n",
    "        steps: list[Step]\n",
    "    \n",
    "    results = {}\n",
    "    failed_folders = []\n",
    "    retry_stats = []\n",
    "    max_retries = 5  # Maximum number of retries before giving up\n",
    "    \n",
    "    # Walk through the Steps directory\n",
    "    for task_name in os.listdir(steps_dir):\n",
    "        task_dir = os.path.join(steps_dir, task_name)\n",
    "        if not os.path.isdir(task_dir):\n",
    "            continue\n",
    "            \n",
    "        results[task_name] = {}\n",
    "        \n",
    "        for run_id in os.listdir(task_dir):\n",
    "            run_dir = os.path.join(task_dir, run_id)\n",
    "            if not os.path.isdir(run_dir):\n",
    "                continue\n",
    "                \n",
    "            output_file = os.path.join(run_dir, \"output.json\")\n",
    "            if not os.path.exists(output_file):\n",
    "                print(f\"No output.json found in {run_dir}\")\n",
    "                continue\n",
    "                \n",
    "            # Read the output.json file\n",
    "            try:\n",
    "                with open(output_file, 'r') as f:\n",
    "                    output_data = json.load(f)\n",
    "                \n",
    "                # Skip empty files or invalid data\n",
    "                if not output_data or not isinstance(output_data, list) or len(output_data) == 0:\n",
    "                    print(f\"Empty or invalid data in {run_dir}/output.json\")\n",
    "                    failed_folders.append(f\"{task_name}/{run_id}\")\n",
    "                    continue\n",
    "                \n",
    "                # Store the original step count\n",
    "                original_step_count = len(output_data)\n",
    "                \n",
    "                # Convert output_data to pretty-printed string for the prompt\n",
    "                output_json_str = json.dumps(output_data, indent=2)\n",
    "                \n",
    "                # Create prompt with the json content and explicitly mention step count\n",
    "                prompt = f\"\"\"\n",
    "                You are a researcher, given the following trace of an AI agent doing ML research challenges:\n",
    "                {output_json_str}\n",
    "                \n",
    "                I want you to return to me a json with step no and what stage of execution is the agent in with the stages being: \n",
    "                1 -> problem/starter code/idea understanding, \n",
    "                2 -> propose idea and implement, \n",
    "                3 -> execute, \n",
    "                4 -> reflect and improve, \n",
    "                5 -> submission\n",
    "                \n",
    "                The response should be a JSON object where the keys are step numbers (as strings) and the values are one of the 5 stages.\n",
    "                \n",
    "                IMPORTANT: The original trace has {original_step_count} steps. Your response MUST contain exactly {original_step_count} steps, numbered from 1 to {original_step_count}.\n",
    "                \n",
    "                Example format:\n",
    "                {{\n",
    "                  \"1\": 1,\n",
    "                  \"2\": 2,\n",
    "                  \"3\": 3,\n",
    "                  \"4\": 4,\n",
    "                  \"5\": 5,\n",
    "                  \"6\": 5,\n",
    "                  \"7\": 5,\n",
    "                  \"8\": 5,\n",
    "                  \"9\": 5,\n",
    "                  \"10\": 5,\n",
    "                  \"11\": 5,\n",
    "                  \"12\": 5,\n",
    "                }}\n",
    "                \n",
    "                Analyze every step in the trace.\n",
    "                \"\"\"\n",
    "                \n",
    "                print(f\"Processing {task_name}/{run_id}...\")\n",
    "                \n",
    "                # Retry loop for handling step count mismatches\n",
    "                retry_count = 0\n",
    "                success = False\n",
    "                \n",
    "                while not success and retry_count < max_retries:\n",
    "                    try:\n",
    "                        # Call the API\n",
    "                        completion = client.beta.chat.completions.parse(\n",
    "                            model=\"gpt-4o-mini\",\n",
    "                            messages=[\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": prompt\n",
    "                                }\n",
    "                            ],\n",
    "                            response_format=MathReasoning\n",
    "                        )\n",
    "                        \n",
    "                        # Extract the response\n",
    "                        response_content = completion.choices[0].message.content\n",
    "                        \n",
    "                        # Parse the response\n",
    "                        response_json = json.loads(response_content)\n",
    "                        \n",
    "                        # Validate step count\n",
    "                        if len(response_json['steps']) != original_step_count:\n",
    "                            retry_count += 1\n",
    "                            print(f\"Step count mismatch in {task_name}/{run_id} (attempt {retry_count}/{max_retries}): Original has {original_step_count} steps, but analysis has {len(response_json)} steps\")\n",
    "                            \n",
    "                            # Add more explicit instructions for the retry\n",
    "                            prompt = f\"\"\"\n",
    "                            You are a researcher, given the following trace of an AI agent doing ML research challenges:\n",
    "                            {output_json_str}\n",
    "                            \n",
    "                            I want you to return to me a json with step no and what stage of execution is the agent in with the stages being: \n",
    "                            1 -> problem/starter code/idea understanding, \n",
    "                            2 -> propose idea and implement, \n",
    "                            3 -> execute, \n",
    "                            4 -> reflect and improve, \n",
    "                            5 -> submission\n",
    "                            \n",
    "                            The response should be a JSON object where the keys are step numbers (as strings) and the values are one of the 5 stages.\n",
    "                            \n",
    "                            CRITICAL ERROR IN PREVIOUS ATTEMPT: You provided {len(response_json)} steps, but the original trace has EXACTLY {original_step_count} steps.\n",
    "                            \n",
    "                            Your response MUST have EXACTLY {original_step_count} steps, with keys from \"1\" to \"{original_step_count}\".\n",
    "                            Make sure to include every step number from 1 to {original_step_count} in your response.\n",
    "                            \n",
    "                            Example format (if there were 12 steps):\n",
    "                            {{\n",
    "                              \"1\": 1,\n",
    "                              \"2\": 2,\n",
    "                              \"3\": 3,\n",
    "                              \"4\": 4,\n",
    "                              \"5\": 5,\n",
    "                              \"6\": 5,\n",
    "                              \"7\": 5,\n",
    "                              \"8\": 5,\n",
    "                              \"9\": 5,\n",
    "                              \"10\": 5,\n",
    "                              \"11\": 5,\n",
    "                              \"12\": 5,\n",
    "                            }}\n",
    "                            \n",
    "                            But your response should have {original_step_count} steps, not 12.\n",
    "                            Analyze every step in the trace and ensure your response has EXACTLY {original_step_count} entries.\n",
    "                            \"\"\"\n",
    "                            \n",
    "                            # Add a small delay before retrying\n",
    "                            time.sleep(2)\n",
    "                        else:\n",
    "                            # If step count matches, we're successful\n",
    "                            success = True\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        retry_count += 1\n",
    "                        print(f\"Error processing {task_name}/{run_id} (attempt {retry_count}/{max_retries}): {str(e)}\")\n",
    "                        time.sleep(2)\n",
    "                \n",
    "                # After the retry loop, check if we had success\n",
    "                if success:\n",
    "                    # Save the analysis result\n",
    "                    analysis_file = os.path.join(run_dir, \"analysis.json\")\n",
    "                    with open(analysis_file, 'w') as f:\n",
    "                        json.dump(response_json, f, indent=2)\n",
    "                        \n",
    "                    results[task_name][run_id] = response_json\n",
    "                    \n",
    "                    # Record retry stats if we needed retries\n",
    "                    if retry_count > 0:\n",
    "                        retry_stats.append(f\"{task_name}/{run_id}: Succeeded after {retry_count} retries\")\n",
    "                        \n",
    "                    print(f\"Analysis saved to {analysis_file}\" + (f\" after {retry_count} retries\" if retry_count > 0 else \"\"))\n",
    "                    \n",
    "                else:\n",
    "                    # All retries failed\n",
    "                    print(f\"Failed to get correct step count for {task_name}/{run_id} after {max_retries} attempts\")\n",
    "                    failed_folders.append(f\"{task_name}/{run_id}\")\n",
    "                    retry_stats.append(f\"{task_name}/{run_id}: Failed after {max_retries} retries\")\n",
    "                \n",
    "                # Add a small delay to avoid rate limiting\n",
    "                time.sleep(1)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file for {task_name}/{run_id}: {str(e)}\")\n",
    "                failed_folders.append(f\"{task_name}/{run_id}\")\n",
    "    \n",
    "    # Save the compiled results\n",
    "    compiled_results_file = os.path.join(steps_dir, \"compiled_analysis.json\")\n",
    "    with open(compiled_results_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Save the list of failed folders\n",
    "    failed_folders_file = os.path.join(steps_dir, \"failed_folders.txt\")\n",
    "    with open(failed_folders_file, 'w') as f:\n",
    "        for folder in failed_folders:\n",
    "            f.write(f\"{folder}\\n\")\n",
    "    \n",
    "    # Save the retry statistics\n",
    "    retry_stats_file = os.path.join(steps_dir, \"retry_stats.txt\")\n",
    "    with open(retry_stats_file, 'w') as f:\n",
    "        for stat in retry_stats:\n",
    "            f.write(f\"{stat}\\n\")\n",
    "    \n",
    "    print(f\"Compiled results saved to {compiled_results_file}\")\n",
    "    print(f\"Failed folders list saved to {failed_folders_file}\")\n",
    "    print(f\"Retry statistics saved to {retry_stats_file}\")\n",
    "    print(f\"Total failed folders: {len(failed_folders)}\")\n",
    "    print(f\"Folders that needed retries: {len(retry_stats)}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing machine_unlearning/0203205627_279760...\n",
      "Analysis saved to Steps/machine_unlearning/0203205627_279760/analysis.json\n",
      "Processing machine_unlearning/0203205627_2107738...\n",
      "Analysis saved to Steps/machine_unlearning/0203205627_2107738/analysis.json\n",
      "Processing machine_unlearning/0204004906_2156777...\n",
      "Analysis saved to Steps/machine_unlearning/0204004906_2156777/analysis.json\n",
      "Processing machine_unlearning/0204021048_1005508...\n",
      "Analysis saved to Steps/machine_unlearning/0204021048_1005508/analysis.json\n",
      "Processing machine_unlearning/0204000906_980483...\n",
      "Analysis saved to Steps/machine_unlearning/0204000906_980483/analysis.json\n",
      "Processing machine_unlearning/0204002434_497948...\n",
      "Analysis saved to Steps/machine_unlearning/0204002434_497948/analysis.json\n",
      "Processing machine_unlearning/0203235437_311468...\n",
      "Analysis saved to Steps/machine_unlearning/0203235437_311468/analysis.json\n",
      "Processing machine_unlearning/0203205627_931197...\n",
      "Analysis saved to Steps/machine_unlearning/0203205627_931197/analysis.json\n",
      "Processing meta-learning/0211164506_234282...\n",
      "Analysis saved to Steps/meta-learning/0211164506_234282/analysis.json\n",
      "Processing meta-learning/0212001115_3514169...\n",
      "Analysis saved to Steps/meta-learning/0212001115_3514169/analysis.json\n",
      "Processing meta-learning/0212223706_984095...\n",
      "Analysis saved to Steps/meta-learning/0212223706_984095/analysis.json\n",
      "Processing meta-learning/0212230702_2432398...\n",
      "Analysis saved to Steps/meta-learning/0212230702_2432398/analysis.json\n",
      "Processing meta-learning/0212021042_3840295...\n",
      "Analysis saved to Steps/meta-learning/0212021042_3840295/analysis.json\n",
      "Processing meta-learning/0211142216_3241844...\n",
      "Analysis saved to Steps/meta-learning/0211142216_3241844/analysis.json\n",
      "Processing meta-learning/0212235435_1003034...\n",
      "Analysis saved to Steps/meta-learning/0212235435_1003034/analysis.json\n",
      "Processing meta-learning/0211192713_3421734...\n",
      "Analysis saved to Steps/meta-learning/0211192713_3421734/analysis.json\n",
      "Processing llm-merging/0121081654...\n",
      "Analysis saved to Steps/llm-merging/0121081654/analysis.json\n",
      "Processing llm-merging/0124071448...\n",
      "Analysis saved to Steps/llm-merging/0124071448/analysis.json\n",
      "Processing llm-merging/0124110854...\n",
      "Analysis saved to Steps/llm-merging/0124110854/analysis.json\n",
      "Processing llm-merging/0122121253...\n",
      "Analysis saved to Steps/llm-merging/0122121253/analysis.json\n",
      "Processing llm-merging/0122132158...\n",
      "Analysis saved to Steps/llm-merging/0122132158/analysis.json\n",
      "Processing llm-merging/0121200118...\n",
      "Analysis saved to Steps/llm-merging/0121200118/analysis.json\n",
      "Processing llm-merging/0121073211...\n",
      "Analysis saved to Steps/llm-merging/0121073211/analysis.json\n",
      "Processing llm-merging/0121214229...\n",
      "Analysis saved to Steps/llm-merging/0121214229/analysis.json\n",
      "Processing backdoor-trigger-recovery/0124032241...\n",
      "Analysis saved to Steps/backdoor-trigger-recovery/0124032241/analysis.json\n",
      "Processing backdoor-trigger-recovery/0121200721...\n",
      "Analysis saved to Steps/backdoor-trigger-recovery/0121200721/analysis.json\n",
      "Processing backdoor-trigger-recovery/0124144709...\n",
      "Analysis saved to Steps/backdoor-trigger-recovery/0124144709/analysis.json\n",
      "Processing backdoor-trigger-recovery/0122092147...\n",
      "Analysis saved to Steps/backdoor-trigger-recovery/0122092147/analysis.json\n",
      "Processing backdoor-trigger-recovery/0121045246...\n",
      "Analysis saved to Steps/backdoor-trigger-recovery/0121045246/analysis.json\n",
      "Processing backdoor-trigger-recovery/0122164524...\n",
      "Analysis saved to Steps/backdoor-trigger-recovery/0122164524/analysis.json\n",
      "Processing backdoor-trigger-recovery/0121105812...\n",
      "Analysis saved to Steps/backdoor-trigger-recovery/0121105812/analysis.json\n",
      "Processing backdoor-trigger-recovery/0122014648...\n",
      "Analysis saved to Steps/backdoor-trigger-recovery/0122014648/analysis.json\n",
      "Processing perception_temporal_action_loc/0130064726...\n",
      "Analysis saved to Steps/perception_temporal_action_loc/0130064726/analysis.json\n",
      "Processing perception_temporal_action_loc/0130104224...\n",
      "Analysis saved to Steps/perception_temporal_action_loc/0130104224/analysis.json\n",
      "Processing perception_temporal_action_loc/0130021740...\n",
      "Analysis saved to Steps/perception_temporal_action_loc/0130021740/analysis.json\n",
      "Processing perception_temporal_action_loc/0130045001...\n",
      "Analysis saved to Steps/perception_temporal_action_loc/0130045001/analysis.json\n",
      "Processing perception_temporal_action_loc/0130093111...\n",
      "Analysis saved to Steps/perception_temporal_action_loc/0130093111/analysis.json\n",
      "Processing perception_temporal_action_loc/0130112112...\n",
      "Analysis saved to Steps/perception_temporal_action_loc/0130112112/analysis.json\n",
      "Processing perception_temporal_action_loc/0130102918...\n",
      "Analysis saved to Steps/perception_temporal_action_loc/0130102918/analysis.json\n",
      "Processing perception_temporal_action_loc/0130102836...\n",
      "Analysis saved to Steps/perception_temporal_action_loc/0130102836/analysis.json\n",
      "Compiled results saved to Steps/compiled_analysis.json\n",
      "Failed folders list saved to Steps/failed_folders.txt\n",
      "Retry statistics saved to Steps/retry_stats.txt\n",
      "Total failed folders: 0\n",
      "Folders that needed retries: 0\n"
     ]
    }
   ],
   "source": [
    "# Process all the files\n",
    "results = process_and_analyze_files(\"Steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_timestamps_to_analysis(steps_dir, data_dir):\n",
    "    # Process all task directories\n",
    "    for task_name in os.listdir(steps_dir):\n",
    "        task_dir = os.path.join(steps_dir, task_name)\n",
    "        if not os.path.isdir(task_dir):\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing task: {task_name}\")\n",
    "        \n",
    "        # For each run directory\n",
    "        for run_id in os.listdir(task_dir):\n",
    "            run_dir = os.path.join(task_dir, run_id)\n",
    "            if not os.path.isdir(run_dir):\n",
    "                continue\n",
    "                \n",
    "            # Find the analysis.json file\n",
    "            analysis_file = os.path.join(run_dir, \"analysis.json\")\n",
    "            if not os.path.exists(analysis_file):\n",
    "                print(f\"  No analysis.json found in {run_dir}\")\n",
    "                continue\n",
    "                \n",
    "            # Find the corresponding trace.json file\n",
    "            trace_file = find_trace_file(data_dir, task_name, run_id)\n",
    "            if not trace_file:\n",
    "                print(f\"  No trace.json found for {task_name}/{run_id}\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"  Processing run: {run_id}\")\n",
    "            \n",
    "            try:\n",
    "                # Load the analysis data\n",
    "                with open(analysis_file, 'r') as f:\n",
    "                    analysis_data = json.load(f)\n",
    "                \n",
    "                # Load the trace data\n",
    "                with open(trace_file, 'r') as f:\n",
    "                    trace_data = json.load(f)\n",
    "                \n",
    "                # Add timestamps to analysis steps\n",
    "                if 'steps' in analysis_data:\n",
    "                    updated_steps = add_timestamps(analysis_data['steps'], trace_data)\n",
    "                    analysis_data['steps'] = updated_steps\n",
    "                else:\n",
    "                    print(f\"  Warning: analysis.json doesn't have 'steps' key in {run_dir}\")\n",
    "                \n",
    "                # Save the updated analysis file\n",
    "                with open(analysis_file, 'w') as f:\n",
    "                    json.dump(analysis_data, f, indent=2)\n",
    "                    \n",
    "                print(f\"  Added timestamps to {analysis_file}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing {run_id}: {str(e)}\")\n",
    "\n",
    "def find_trace_file(data_dir, task_name, run_id):\n",
    "    \"\"\"Find the trace.json file for the corresponding task/run_id\"\"\"\n",
    "    # Based on your directory structure from the screenshots\n",
    "    path_pattern = os.path.join(data_dir, task_name, \"*\", run_id, \"env_log\", \"trace.json\")\n",
    "    \n",
    "    # Use glob to find matching files\n",
    "    matching_files = glob.glob(path_pattern)\n",
    "    \n",
    "    if matching_files:\n",
    "        return matching_files[0]  # Return the first match\n",
    "    \n",
    "    # Try with a more flexible pattern if the first one fails\n",
    "    path_pattern = os.path.join(data_dir, task_name, \"**\", run_id, \"**\", \"trace.json\")\n",
    "    matching_files = glob.glob(path_pattern, recursive=True)\n",
    "    \n",
    "    if matching_files:\n",
    "        return matching_files[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "def add_timestamps(analysis_steps, trace_data):\n",
    "    \"\"\"Add timestamps from trace_data to analysis_steps\"\"\"\n",
    "    # Create a mapping of step index to timestamp\n",
    "    timestamps = {}\n",
    "    \n",
    "    # Check if trace_data is a dictionary with 'steps' key\n",
    "    if isinstance(trace_data, dict) and 'steps' in trace_data:\n",
    "        trace_steps = trace_data['steps']\n",
    "    else:\n",
    "        # Assume trace_data is already the steps array\n",
    "        trace_steps = trace_data\n",
    "    \n",
    "    for i, step in enumerate(trace_steps, 1):  # Start index from 1\n",
    "        if 'timestamp' in step:\n",
    "            timestamps[i] = step['timestamp']\n",
    "    \n",
    "    # Add timestamps to analysis steps\n",
    "    for step in analysis_steps:\n",
    "        step_id = step['Step_ID']\n",
    "        if step_id in timestamps:\n",
    "            step['Timestamp'] = timestamps[step_id]\n",
    "        else:\n",
    "            print(f\"    Warning: No timestamp found for step {step_id}\")\n",
    "    \n",
    "    return analysis_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing task: machine_unlearning\n",
      "  Processing run: 0203205627_279760\n",
      "  Added timestamps to Steps/machine_unlearning/0203205627_279760/analysis.json\n",
      "  Processing run: 0203205627_2107738\n",
      "  Added timestamps to Steps/machine_unlearning/0203205627_2107738/analysis.json\n",
      "  Processing run: 0204004906_2156777\n",
      "  Added timestamps to Steps/machine_unlearning/0204004906_2156777/analysis.json\n",
      "  Processing run: 0204021048_1005508\n",
      "  Added timestamps to Steps/machine_unlearning/0204021048_1005508/analysis.json\n",
      "  Processing run: 0204000906_980483\n",
      "  Added timestamps to Steps/machine_unlearning/0204000906_980483/analysis.json\n",
      "  Processing run: 0204002434_497948\n",
      "  Added timestamps to Steps/machine_unlearning/0204002434_497948/analysis.json\n",
      "  Processing run: 0203235437_311468\n",
      "  Added timestamps to Steps/machine_unlearning/0203235437_311468/analysis.json\n",
      "  Processing run: 0203205627_931197\n",
      "  Added timestamps to Steps/machine_unlearning/0203205627_931197/analysis.json\n",
      "Processing task: meta-learning\n",
      "  Processing run: 0211164506_234282\n",
      "  Added timestamps to Steps/meta-learning/0211164506_234282/analysis.json\n",
      "  Processing run: 0212001115_3514169\n",
      "  Added timestamps to Steps/meta-learning/0212001115_3514169/analysis.json\n",
      "  Processing run: 0212223706_984095\n",
      "  Added timestamps to Steps/meta-learning/0212223706_984095/analysis.json\n",
      "  Processing run: 0212230702_2432398\n",
      "  Added timestamps to Steps/meta-learning/0212230702_2432398/analysis.json\n",
      "  Processing run: 0212021042_3840295\n",
      "  Added timestamps to Steps/meta-learning/0212021042_3840295/analysis.json\n",
      "  Processing run: 0211142216_3241844\n",
      "  Added timestamps to Steps/meta-learning/0211142216_3241844/analysis.json\n",
      "  Processing run: 0212235435_1003034\n",
      "  Added timestamps to Steps/meta-learning/0212235435_1003034/analysis.json\n",
      "  Processing run: 0211192713_3421734\n",
      "  Added timestamps to Steps/meta-learning/0211192713_3421734/analysis.json\n",
      "Processing task: llm-merging\n",
      "  Processing run: 0121081654\n",
      "  Added timestamps to Steps/llm-merging/0121081654/analysis.json\n",
      "  Processing run: 0124071448\n",
      "  Added timestamps to Steps/llm-merging/0124071448/analysis.json\n",
      "  Processing run: 0124110854\n",
      "  Added timestamps to Steps/llm-merging/0124110854/analysis.json\n",
      "  Processing run: 0122121253\n",
      "  Added timestamps to Steps/llm-merging/0122121253/analysis.json\n",
      "  Processing run: 0122132158\n",
      "  Added timestamps to Steps/llm-merging/0122132158/analysis.json\n",
      "  Processing run: 0121200118\n",
      "  Added timestamps to Steps/llm-merging/0121200118/analysis.json\n",
      "  Processing run: 0121073211\n",
      "  Added timestamps to Steps/llm-merging/0121073211/analysis.json\n",
      "  Processing run: 0121214229\n",
      "  Added timestamps to Steps/llm-merging/0121214229/analysis.json\n",
      "Processing task: backdoor-trigger-recovery\n",
      "  Processing run: 0124032241\n",
      "  Added timestamps to Steps/backdoor-trigger-recovery/0124032241/analysis.json\n",
      "  Processing run: 0121200721\n",
      "  Added timestamps to Steps/backdoor-trigger-recovery/0121200721/analysis.json\n",
      "  Processing run: 0124144709\n",
      "  Added timestamps to Steps/backdoor-trigger-recovery/0124144709/analysis.json\n",
      "  Processing run: 0122092147\n",
      "  Added timestamps to Steps/backdoor-trigger-recovery/0122092147/analysis.json\n",
      "  Processing run: 0121045246\n",
      "  Added timestamps to Steps/backdoor-trigger-recovery/0121045246/analysis.json\n",
      "  Processing run: 0122164524\n",
      "  Added timestamps to Steps/backdoor-trigger-recovery/0122164524/analysis.json\n",
      "  Processing run: 0121105812\n",
      "  Added timestamps to Steps/backdoor-trigger-recovery/0121105812/analysis.json\n",
      "  Processing run: 0122014648\n",
      "  Added timestamps to Steps/backdoor-trigger-recovery/0122014648/analysis.json\n",
      "Processing task: perception_temporal_action_loc\n",
      "  Processing run: 0130064726\n",
      "  Added timestamps to Steps/perception_temporal_action_loc/0130064726/analysis.json\n",
      "  Processing run: 0130104224\n",
      "  Added timestamps to Steps/perception_temporal_action_loc/0130104224/analysis.json\n",
      "  Processing run: 0130021740\n",
      "  Added timestamps to Steps/perception_temporal_action_loc/0130021740/analysis.json\n",
      "  Processing run: 0130045001\n",
      "  Added timestamps to Steps/perception_temporal_action_loc/0130045001/analysis.json\n",
      "  Processing run: 0130093111\n",
      "  Added timestamps to Steps/perception_temporal_action_loc/0130093111/analysis.json\n",
      "  Processing run: 0130112112\n",
      "  Added timestamps to Steps/perception_temporal_action_loc/0130112112/analysis.json\n",
      "  Processing run: 0130102918\n",
      "  Added timestamps to Steps/perception_temporal_action_loc/0130102918/analysis.json\n",
      "  Processing run: 0130102836\n",
      "  Added timestamps to Steps/perception_temporal_action_loc/0130102836/analysis.json\n",
      "Timestamp addition complete!\n"
     ]
    }
   ],
   "source": [
    "steps_dir = \"Steps\"\n",
    "data_dir = \"data\" \n",
    "\n",
    "add_timestamps_to_analysis(steps_dir, data_dir)\n",
    "print(\"Timestamp addition complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_stages_in_json(file_path):\n",
    "    # Stage mapping (text to numeric)\n",
    "    reverse_mapping = {\n",
    "        \"problem/starter code/idea understanding\": \"1\",\n",
    "        \"propose idea and implement\": \"2\",\n",
    "        \"execute\": \"3\",\n",
    "        \"reflect and improve\": \"4\",\n",
    "        \"submission\": \"5\"\n",
    "    }\n",
    "    \n",
    "    # Load the JSON data\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Check if 'steps' exists\n",
    "    if 'steps' not in data:\n",
    "        print(f\"Error: No 'steps' key found in {file_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Apply reverse mapping to convert text stages to numeric\n",
    "    changes_made = 0\n",
    "    for step in data['steps']:\n",
    "        if 'Stage' in step and step['Stage'] in reverse_mapping:\n",
    "            step['Stage'] = reverse_mapping[step['Stage']]\n",
    "            changes_made += 1\n",
    "    \n",
    "    # Save the updated JSON file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    print(f\"Fixed {changes_made} stages in {file_path}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 50 stages in Steps/machine_unlearning/0204002434_497948/analysis.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fix stages in analysis.json Only Run in case API returns string instead of numbers\n",
    "#file_path = \"Steps/machine_unlearning/0204002434_497948/analysis.json\"\n",
    "#fix_stages_in_json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Define stage names for reference\n",
    "stage_names = {\n",
    "    1: \"understand\",\n",
    "    2: \"implement\",\n",
    "    3: \"execute\",\n",
    "    4: \"improve\",\n",
    "    5: \"submit\"\n",
    "}\n",
    "\n",
    "def extract_metadata_from_path(file_path):\n",
    "    \"\"\"Extract task and run_id from file path\"\"\"\n",
    "    # Expected path format: Steps/task_name/run_id/analysis.json\n",
    "    path_parts = file_path.split(os.sep)\n",
    "    \n",
    "    # Find steps index (might be \"Steps\" or lowercase \"steps\")\n",
    "    steps_index = -1\n",
    "    for i, part in enumerate(path_parts):\n",
    "        if part.lower() == \"steps\":\n",
    "            steps_index = i\n",
    "            break\n",
    "    \n",
    "    # Get task name (folder right after \"Steps\")\n",
    "    task = path_parts[steps_index + 1] if steps_index >= 0 and steps_index + 1 < len(path_parts) else \"unknown\"\n",
    "    \n",
    "    # The run_id is typically the folder containing the analysis.json file\n",
    "    run_id = path_parts[-2] if len(path_parts) >= 2 else \"unknown\"\n",
    "    \n",
    "    return task, run_id\n",
    "\n",
    "def process_stage_transitions(steps):\n",
    "    \"\"\"Process steps to identify stage transitions and durations\"\"\"\n",
    "    # Sort steps by ID to ensure chronological order\n",
    "    steps = sorted(steps, key=lambda x: x.get('Step_ID', 0))\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    transitions = []\n",
    "    current_stage = None\n",
    "    start_step = None\n",
    "    start_time = None\n",
    "    last_step_id = None\n",
    "    last_step_time = None\n",
    "    \n",
    "    for step in steps:\n",
    "        # Convert stage to int (fix: ensure stage is an int)\n",
    "        stage_str = step.get('Stage')\n",
    "        if stage_str is None or step.get('Step_ID') is None:\n",
    "            continue  # Skip steps with missing data\n",
    "            \n",
    "        # Convert stage to int\n",
    "        try:\n",
    "            stage = int(stage_str)\n",
    "        except (ValueError, TypeError):\n",
    "            continue  # Skip steps with invalid stage values\n",
    "        \n",
    "        step_id = step.get('Step_ID')\n",
    "        step_time = step.get('Timestamp')\n",
    "        \n",
    "        # Keep track of the last step in the current stage\n",
    "        if stage == current_stage:\n",
    "            last_step_id = step_id\n",
    "            last_step_time = step_time\n",
    "        \n",
    "        # When stage changes, record the previous stage block\n",
    "        elif current_stage is not None:\n",
    "            # For stages with only a single step:\n",
    "            if start_step == last_step_id:\n",
    "                # If this is not the first step in the entire process\n",
    "                prev_step_time = None\n",
    "                for prev_step in steps:\n",
    "                    if prev_step['Step_ID'] == start_step - 1:\n",
    "                        prev_step_time = prev_step.get('Timestamp')\n",
    "                        break\n",
    "                \n",
    "                # If we found the previous step, use its timestamp as the start time\n",
    "                if prev_step_time is not None and last_step_time is not None:\n",
    "                    duration = last_step_time - prev_step_time\n",
    "                else:\n",
    "                    # Fallback: estimate based on the current step\n",
    "                    duration = 5  # default seconds for a single step with no context\n",
    "            else:\n",
    "                # For multi-step stages, calculate duration\n",
    "                duration = last_step_time - start_time if (start_time is not None and last_step_time is not None) else None\n",
    "            \n",
    "            # Add the previous stage block\n",
    "            transitions.append({\n",
    "                'stage': current_stage,\n",
    "                'start_step': start_step,\n",
    "                'end_step': last_step_id,\n",
    "                'step_count': last_step_id - start_step + 1,\n",
    "                'start_time': start_time,\n",
    "                'end_time': last_step_time,\n",
    "                'duration': duration\n",
    "            })\n",
    "            \n",
    "            # Start a new stage block\n",
    "            current_stage = stage\n",
    "            start_step = step_id\n",
    "            \n",
    "            # Find the timestamp of the previous step to use as start_time\n",
    "            prev_step_time = None\n",
    "            for prev_step in steps:\n",
    "                if prev_step['Step_ID'] == start_step - 1:\n",
    "                    prev_step_time = prev_step.get('Timestamp')\n",
    "                    break\n",
    "            \n",
    "            # Use previous step's timestamp as start_time\n",
    "            start_time = prev_step_time\n",
    "            \n",
    "            last_step_id = step_id\n",
    "            last_step_time = step_time\n",
    "        \n",
    "        # Initialize the first stage\n",
    "        else:\n",
    "            current_stage = stage\n",
    "            start_step = step_id\n",
    "            \n",
    "            # For the first step, use a reasonable time before\n",
    "            if step_time is not None:\n",
    "                # Estimate start time as 5 seconds before first timestamp\n",
    "                start_time = step_time - 5  # assuming the first step took 5 seconds\n",
    "            else:\n",
    "                start_time = None\n",
    "                \n",
    "            last_step_id = step_id\n",
    "            last_step_time = step_time\n",
    "    \n",
    "    # Add the final stage block\n",
    "    if current_stage is not None:\n",
    "        # For single step final stage\n",
    "        if start_step == last_step_id:\n",
    "            # Try to find the previous step timestamp\n",
    "            prev_step_time = None\n",
    "            for prev_step in steps:\n",
    "                if prev_step['Step_ID'] == start_step - 1:\n",
    "                    prev_step_time = prev_step.get('Timestamp')\n",
    "                    break\n",
    "            \n",
    "            # Calculate duration from previous step to current\n",
    "            if prev_step_time is not None and last_step_time is not None:\n",
    "                duration = last_step_time - prev_step_time\n",
    "            else:\n",
    "                # If no previous step, estimate a reasonable duration\n",
    "                duration = 5  # default seconds\n",
    "        else:\n",
    "            # For multi-step stages\n",
    "            duration = last_step_time - start_time if (start_time is not None and last_step_time is not None) else None\n",
    "        \n",
    "        transitions.append({\n",
    "            'stage': current_stage,\n",
    "            'start_step': start_step,\n",
    "            'end_step': last_step_id,\n",
    "            'step_count': last_step_id - start_step + 1,\n",
    "            'start_time': start_time,\n",
    "            'end_time': last_step_time,\n",
    "            'duration': duration\n",
    "        })\n",
    "    \n",
    "    return transitions\n",
    "\n",
    "def calculate_stage_summaries(transitions):\n",
    "    \"\"\"Calculate summaries by stage from transitions data\"\"\"\n",
    "    stage_counts = {}\n",
    "    stage_durations = {}\n",
    "    \n",
    "    for t in transitions:\n",
    "        stage = t.get('stage')\n",
    "        if stage is None:\n",
    "            continue\n",
    "            \n",
    "        # Accumulate counts and durations (using int stage as key)\n",
    "        stage_counts[stage] = stage_counts.get(stage, 0) + t.get('step_count', 0)\n",
    "        if t.get('duration') is not None:\n",
    "            stage_durations[stage] = stage_durations.get(stage, 0) + t.get('duration')\n",
    "    \n",
    "    return stage_counts, stage_durations\n",
    "\n",
    "def process_analysis_file(file_path):\n",
    "    \"\"\"Process a single analysis.json file and return summary data\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Extract task and run_id from file path\n",
    "        task, run_id = extract_metadata_from_path(file_path)\n",
    "        \n",
    "        # Process the steps to get transitions\n",
    "        steps = data.get('steps', [])\n",
    "        transitions = process_stage_transitions(steps)\n",
    "        \n",
    "        # Calculate stage summaries\n",
    "        stage_counts, stage_durations = calculate_stage_summaries(transitions)\n",
    "        \n",
    "        # Create the summary object\n",
    "        summary = {\n",
    "            \"task\": task,\n",
    "            \"model\": \"claude\",  # Always claude as specified\n",
    "            \"run_id\": run_id\n",
    "        }\n",
    "        \n",
    "        # Add stage information (using int stages)\n",
    "        for stage in range(1, 6):\n",
    "            summary[f\"stage{stage}_time\"] = stage_durations.get(stage, 0)  # In seconds\n",
    "            summary[f\"stage{stage}_steps\"] = stage_counts.get(stage, 0)\n",
    "        \n",
    "        # Add total information\n",
    "        summary[\"total_time\"] = sum(duration for duration in stage_durations.values() if duration is not None)\n",
    "        summary[\"total_steps\"] = sum(stage_counts.values())\n",
    "        \n",
    "        # Add transition information for deeper analysis\n",
    "        summary[\"transitions\"] = transitions\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def find_all_analysis_files(root_dir):\n",
    "    \"\"\"Find all analysis.json files in the directory structure\"\"\"\n",
    "    return glob.glob(os.path.join(root_dir, \"**\", \"analysis.json\"), recursive=True)\n",
    "\n",
    "def create_summary_jsonl(summaries, output_file):\n",
    "    \"\"\"Create a JSONL file from the summaries, excluding the transitions field\"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        for summary in summaries:\n",
    "            # Create a copy without the transitions field for the JSONL output\n",
    "            summary_copy = {k: v for k, v in summary.items() if k != \"transitions\"}\n",
    "            f.write(json.dumps(summary_copy) + '\\n')\n",
    "\n",
    "def main(root_dir, output_file):\n",
    "    \"\"\"Process all analysis files and create JSONL output\"\"\"\n",
    "    # Find all analysis files\n",
    "    analysis_files = find_all_analysis_files(root_dir)\n",
    "    print(f\"Found {len(analysis_files)} analysis files\")\n",
    "    \n",
    "    # Process each file and collect summaries\n",
    "    summaries = []\n",
    "    for file_path in analysis_files:\n",
    "        print(f\"Processing {file_path}\")\n",
    "        summary = process_analysis_file(file_path)\n",
    "        if summary:\n",
    "            summaries.append(summary)\n",
    "    \n",
    "    # Write the summaries to JSONL file (without transitions field)\n",
    "    create_summary_jsonl(summaries, output_file)\n",
    "    print(f\"Wrote {len(summaries)} summaries to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 analysis files\n",
      "Processing Steps/machine_unlearning/0203205627_279760/analysis.json\n",
      "Processing Steps/machine_unlearning/0203205627_2107738/analysis.json\n",
      "Processing Steps/machine_unlearning/0204004906_2156777/analysis.json\n",
      "Processing Steps/machine_unlearning/0204021048_1005508/analysis.json\n",
      "Processing Steps/machine_unlearning/0204000906_980483/analysis.json\n",
      "Processing Steps/machine_unlearning/0204002434_497948/analysis.json\n",
      "Processing Steps/machine_unlearning/0203235437_311468/analysis.json\n",
      "Processing Steps/machine_unlearning/0203205627_931197/analysis.json\n",
      "Processing Steps/meta-learning/0211164506_234282/analysis.json\n",
      "Processing Steps/meta-learning/0212001115_3514169/analysis.json\n",
      "Processing Steps/meta-learning/0212223706_984095/analysis.json\n",
      "Processing Steps/meta-learning/0212230702_2432398/analysis.json\n",
      "Processing Steps/meta-learning/0212021042_3840295/analysis.json\n",
      "Processing Steps/meta-learning/0211142216_3241844/analysis.json\n",
      "Processing Steps/meta-learning/0212235435_1003034/analysis.json\n",
      "Processing Steps/meta-learning/0211192713_3421734/analysis.json\n",
      "Processing Steps/llm-merging/0121081654/analysis.json\n",
      "Processing Steps/llm-merging/0124071448/analysis.json\n",
      "Processing Steps/llm-merging/0124110854/analysis.json\n",
      "Processing Steps/llm-merging/0122121253/analysis.json\n",
      "Processing Steps/llm-merging/0122132158/analysis.json\n",
      "Processing Steps/llm-merging/0121200118/analysis.json\n",
      "Processing Steps/llm-merging/0121073211/analysis.json\n",
      "Processing Steps/llm-merging/0121214229/analysis.json\n",
      "Processing Steps/backdoor-trigger-recovery/0124032241/analysis.json\n",
      "Processing Steps/backdoor-trigger-recovery/0121200721/analysis.json\n",
      "Processing Steps/backdoor-trigger-recovery/0124144709/analysis.json\n",
      "Processing Steps/backdoor-trigger-recovery/0122092147/analysis.json\n",
      "Processing Steps/backdoor-trigger-recovery/0121045246/analysis.json\n",
      "Processing Steps/backdoor-trigger-recovery/0122164524/analysis.json\n",
      "Processing Steps/backdoor-trigger-recovery/0121105812/analysis.json\n",
      "Processing Steps/backdoor-trigger-recovery/0122014648/analysis.json\n",
      "Processing Steps/perception_temporal_action_loc/0130064726/analysis.json\n",
      "Processing Steps/perception_temporal_action_loc/0130104224/analysis.json\n",
      "Processing Steps/perception_temporal_action_loc/0130021740/analysis.json\n",
      "Processing Steps/perception_temporal_action_loc/0130045001/analysis.json\n",
      "Processing Steps/perception_temporal_action_loc/0130093111/analysis.json\n",
      "Processing Steps/perception_temporal_action_loc/0130112112/analysis.json\n",
      "Processing Steps/perception_temporal_action_loc/0130102918/analysis.json\n",
      "Processing Steps/perception_temporal_action_loc/0130102836/analysis.json\n",
      "Wrote 40 summaries to stage_summaries.jsonl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ROOT_DIR = \"Steps\"\n",
    "OUTPUT_FILE = \"stage_summaries.jsonl\"\n",
    "VIS_DIR = \"visualizations\"\n",
    "\n",
    "main(ROOT_DIR, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "def process_analysis_json(file_path, output_dir):\n",
    "    \"\"\"\n",
    "    Process a single analysis.json file and create a timeline visualization.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the analysis.json file\n",
    "        output_dir: Directory to save the output figure\n",
    "    \"\"\"\n",
    "    # Extract task name from file path\n",
    "    # The path format is Steps/task_name/timestamp/analysis.json\n",
    "    path_parts = file_path.split(os.sep)\n",
    "    task_name = path_parts[-3] if len(path_parts) >= 3 else \"Unknown_Task\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load the JSON data\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Check if data has the expected structure\n",
    "    if 'steps' not in data:\n",
    "        print(f\"Warning: {file_path} does not contain 'steps' key, skipping\")\n",
    "        return\n",
    "    \n",
    "    # Process the data\n",
    "    steps = data['steps']\n",
    "    \n",
    "    # Define stage names\n",
    "    stage_names = {\n",
    "        1: \"understand\",\n",
    "        2: \"implement\",\n",
    "        3: \"execute\",\n",
    "        4: \"improve\",\n",
    "        5: \"submit\"\n",
    "    }\n",
    "    \n",
    "    # Define stage colors\n",
    "    stage_colors = {\n",
    "        1: \"#b3e6b3\",  # Light green\n",
    "        2: \"#b3b3e6\",  # Light blue\n",
    "        3: \"#e6b3b3\",  # Light red\n",
    "        4: \"#e6e6b3\",  # Light yellow\n",
    "        5: \"#e6b3e6\"   # Light purple\n",
    "    }\n",
    "    \n",
    "    # Group steps by stage transitions\n",
    "    transitions = []\n",
    "    current_stage = None\n",
    "    start_step = None\n",
    "    start_time = None\n",
    "    last_step_id = None\n",
    "    last_step_time = None\n",
    "    \n",
    "    for step in steps:\n",
    "        # Skip steps without Stage information\n",
    "        if 'Stage' not in step:\n",
    "            continue\n",
    "            \n",
    "        stage = step['Stage']\n",
    "        step_time = step.get('Timestamp')\n",
    "        \n",
    "        # Keep track of the last step in the current stage\n",
    "        if stage == current_stage:\n",
    "            last_step_id = step['Step_ID']\n",
    "            last_step_time = step_time\n",
    "        \n",
    "        # When stage changes, record the previous stage block\n",
    "        elif current_stage is not None:\n",
    "            # For stages with only a single step:\n",
    "            if start_step == last_step_id:\n",
    "                # If this is not the first step in the entire process\n",
    "                prev_step_time = None\n",
    "                for prev_step in steps:\n",
    "                    if prev_step['Step_ID'] == start_step - 1:\n",
    "                        prev_step_time = prev_step.get('Timestamp')\n",
    "                        break\n",
    "                \n",
    "                # If we found the previous step, use its timestamp as the start time\n",
    "                if prev_step_time is not None and last_step_time is not None:\n",
    "                    duration = last_step_time - prev_step_time\n",
    "                else:\n",
    "                    # Fallback: estimate based on the current step\n",
    "                    duration = 5  # default seconds for a single step with no context\n",
    "            else:\n",
    "                # For multi-step stages, the duration is from the prior stage's last step\n",
    "                # to this stage's last step\n",
    "                duration = last_step_time - start_time if (start_time is not None and last_step_time is not None) else None\n",
    "            \n",
    "            # Add the previous stage block\n",
    "            transitions.append({\n",
    "                'stage': current_stage,\n",
    "                'start_step': start_step,\n",
    "                'end_step': last_step_id,\n",
    "                'position': len(transitions),\n",
    "                'start_time': start_time,\n",
    "                'end_time': last_step_time,\n",
    "                'duration': duration\n",
    "            })\n",
    "            \n",
    "            # Start a new stage block\n",
    "            current_stage = stage\n",
    "            start_step = step['Step_ID']\n",
    "            \n",
    "            # Find the timestamp of the previous step to use as start_time\n",
    "            prev_step_time = None\n",
    "            for prev_step in steps:\n",
    "                if prev_step['Step_ID'] == start_step - 1:\n",
    "                    prev_step_time = prev_step.get('Timestamp')\n",
    "                    break\n",
    "            \n",
    "            # Use previous step's timestamp as start_time\n",
    "            start_time = prev_step_time\n",
    "            \n",
    "            last_step_id = step['Step_ID']\n",
    "            last_step_time = step_time\n",
    "        \n",
    "        # Initialize the first stage\n",
    "        else:\n",
    "            current_stage = stage\n",
    "            start_step = step['Step_ID']\n",
    "            \n",
    "            # For the first step, use a reasonable time before\n",
    "            if step_time is not None:\n",
    "                # Estimate start time as 5 seconds before first timestamp\n",
    "                start_time = step_time - 5  # assuming the first step took 5 seconds\n",
    "            else:\n",
    "                start_time = None\n",
    "                \n",
    "            last_step_id = step['Step_ID']\n",
    "            last_step_time = step_time\n",
    "    \n",
    "    # Add the final stage block\n",
    "    if current_stage is not None:\n",
    "        # For single step final stage\n",
    "        if start_step == last_step_id:\n",
    "            # Try to find the previous step timestamp\n",
    "            prev_step_time = None\n",
    "            for prev_step in steps:\n",
    "                if prev_step['Step_ID'] == start_step - 1:\n",
    "                    prev_step_time = prev_step.get('Timestamp')\n",
    "                    break\n",
    "            \n",
    "            # Calculate duration from previous step to current\n",
    "            if prev_step_time is not None and last_step_time is not None:\n",
    "                duration = last_step_time - prev_step_time\n",
    "            else:\n",
    "                # If no previous step, estimate a reasonable duration\n",
    "                duration = 5  # default seconds\n",
    "        else:\n",
    "            # For multi-step stages\n",
    "            duration = last_step_time - start_time if (start_time is not None and last_step_time is not None) else None\n",
    "        \n",
    "        transitions.append({\n",
    "            'stage': current_stage,\n",
    "            'start_step': start_step,\n",
    "            'end_step': last_step_id,\n",
    "            'position': len(transitions),\n",
    "            'start_time': start_time,\n",
    "            'end_time': last_step_time,\n",
    "            'duration': duration\n",
    "        })\n",
    "    \n",
    "    # Count steps in each stage for proportional sizing\n",
    "    for t in transitions:\n",
    "        t['step_count'] = t['end_step'] - t['start_step'] + 1\n",
    "    \n",
    "    # Create the figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))  # Taller to accommodate time labels\n",
    "    \n",
    "    # Setup the plot\n",
    "    y_pos = 0.6  # Move up to make room for time labels\n",
    "    height = 0.6\n",
    "    total_width = 1.0\n",
    "    \n",
    "    # Calculate total duration for scaling\n",
    "    total_duration = sum(t['duration'] for t in transitions if t['duration'] is not None)\n",
    "    if total_duration <= 0:\n",
    "        # Fallback to step-based sizing if durations are not available\n",
    "        total_steps = sum(t['step_count'] for t in transitions)\n",
    "        use_duration_scaling = False\n",
    "    else:\n",
    "        use_duration_scaling = True\n",
    "    \n",
    "    block_positions = []\n",
    "    current_pos = 0\n",
    "    \n",
    "    # Draw the timeline blocks\n",
    "    min_block_width = 0.02  # Minimum width for visibility\n",
    "    adjusted_total_width = total_width - (min_block_width * len([t for t in transitions if t['duration'] is not None and (t['duration'] / total_duration) * total_width < min_block_width]))\n",
    "    \n",
    "    for i, block in enumerate(transitions):\n",
    "        # Calculate width proportional to duration instead of step count\n",
    "        if use_duration_scaling and block['duration'] is not None:\n",
    "            # Calculate the proportional width\n",
    "            proportional_width = (block['duration'] / total_duration) * total_width\n",
    "            \n",
    "            # Apply minimum width if needed\n",
    "            if proportional_width < min_block_width:\n",
    "                block_width = min_block_width\n",
    "            else:\n",
    "                # Scale the remaining width to account for minimum widths\n",
    "                adjustment_factor = adjusted_total_width / total_width if adjusted_total_width > 0 else 1\n",
    "                block_width = proportional_width * adjustment_factor\n",
    "        else:\n",
    "            # Fallback to step count if duration is not available\n",
    "            block_width = (block['step_count'] / total_steps) * total_width\n",
    "            # Apply minimum width\n",
    "            if block_width < min_block_width:\n",
    "                block_width = min_block_width\n",
    "        \n",
    "        # Create rectangle\n",
    "        rect = patches.Rectangle(\n",
    "            (current_pos, y_pos - height/2),\n",
    "            block_width,\n",
    "            height,\n",
    "            linewidth=1,\n",
    "            edgecolor='black',\n",
    "            facecolor=stage_colors.get(block['stage'], \"#cccccc\"),  # Default to gray if stage not in colors\n",
    "            alpha=0.8\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add step count to each block\n",
    "        mid_point = current_pos + block_width/2\n",
    "        ax.text(\n",
    "            mid_point, y_pos, \n",
    "            f\"{block['step_count']}\", \n",
    "            ha='center', va='center', \n",
    "            fontsize=9, \n",
    "            fontweight='bold',\n",
    "            color='black'\n",
    "        )\n",
    "        \n",
    "        # Add time duration below each block\n",
    "        if block['duration'] is not None:\n",
    "            # Format duration (in seconds)\n",
    "            duration_seconds = block['duration']\n",
    "            if duration_seconds >= 3600:  # More than an hour\n",
    "                hours = int(duration_seconds // 3600)\n",
    "                minutes = int((duration_seconds % 3600) // 60)\n",
    "                duration_str = f\"{hours}h {minutes}m\"\n",
    "            elif duration_seconds >= 60:  # More than a minute\n",
    "                minutes = int(duration_seconds // 60)\n",
    "                seconds = int(duration_seconds % 60)\n",
    "                duration_str = f\"{minutes}m {seconds}s\"\n",
    "            else:\n",
    "                duration_str = f\"{int(duration_seconds)}s\"\n",
    "            \n",
    "            # Add time label with rotation for better fit\n",
    "            # Position it slightly below the block\n",
    "            ax.text(\n",
    "                mid_point, y_pos - height/2 - 0.1, \n",
    "                duration_str, \n",
    "                ha='center', va='top', \n",
    "                fontsize=8,\n",
    "                color='black',\n",
    "                rotation=90 if block_width < 0.05 else 0  # Rotate label if block is narrow\n",
    "            )\n",
    "        \n",
    "        # Store position for timeline\n",
    "        block_positions.append((current_pos, block_width))\n",
    "        \n",
    "        # Move to next position\n",
    "        current_pos += block_width\n",
    "    \n",
    "    # Draw timeline line - moved further down to avoid overlapping with labels\n",
    "    timeline_y_position = y_pos - height/2 - 0.35  # Increased this value to move line down\n",
    "    ax.plot([0, total_width], [timeline_y_position, timeline_y_position], 'k-', alpha=0.5)\n",
    "    \n",
    "    # Set up the axes\n",
    "    ax.set_xlim(-0.05, total_width + 0.1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Add a legend for stages\n",
    "    legend_elements = []\n",
    "    for stage in sorted(set(t['stage'] for t in transitions)):\n",
    "        if stage in stage_names:\n",
    "            legend_elements.append(\n",
    "                patches.Patch(facecolor=stage_colors.get(stage, \"#cccccc\"), \n",
    "                              edgecolor='black', \n",
    "                              label=f'Stage: {stage_names.get(stage, \"unknown\")}')\n",
    "            )\n",
    "    plt.legend(handles=legend_elements, loc='upper center', \n",
    "               bbox_to_anchor=(0.5, -0.15), ncol=5)\n",
    "    \n",
    "    # Format task name for display\n",
    "    display_task_name = task_name.replace('_', ' ').title()\n",
    "    \n",
    "    # Add title\n",
    "    plt.title(f'{display_task_name} Stage Timeline', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add context about scaling method\n",
    "    if use_duration_scaling:\n",
    "        plt.figtext(0.5, 0.01, \n",
    "                    \"Note: Block widths are proportional to time duration in each stage, with minimum size for visibility.\\nDuration times shown below each block.\\nNumber in each block is the number of steps in that stage.\", \n",
    "                    ha='center', fontsize=8)\n",
    "    else:\n",
    "        plt.figtext(0.5, 0.01, \n",
    "                    \"Note: Block widths are proportional to number of steps in each stage.\\nDuration times shown below each block.\", \n",
    "                    ha='center', fontsize=8)\n",
    "    \n",
    "    # Tight layout\n",
    "    plt.tight_layout(rect=[0, 0.15, 1, 0.95])\n",
    "    \n",
    "    # Generate output filename\n",
    "    output_filename = os.path.join(output_dir, f\"{task_name}_stage_timeline.png\")\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f\"Generated timeline for {task_name} at {output_filename}\")\n",
    "\n",
    "def find_analysis_files(root_dir):\n",
    "    \"\"\"\n",
    "    Find all analysis.json files under the root directory\n",
    "    \n",
    "    Args:\n",
    "        root_dir: Root directory to start the search\n",
    "        \n",
    "    Returns:\n",
    "        List of paths to analysis.json files\n",
    "    \"\"\"\n",
    "    analysis_files = []\n",
    "    \n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        if 'analysis.json' in filenames:\n",
    "            analysis_files.append(os.path.join(dirpath, 'analysis.json'))\n",
    "    \n",
    "    return analysis_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 analysis.json files to process\n",
      "Generated timeline for machine_unlearning at Steps/machine_unlearning/0203205627_279760/machine_unlearning_stage_timeline.png\n",
      "Generated timeline for machine_unlearning at Steps/machine_unlearning/0203205627_2107738/machine_unlearning_stage_timeline.png\n",
      "Generated timeline for machine_unlearning at Steps/machine_unlearning/0204004906_2156777/machine_unlearning_stage_timeline.png\n",
      "Generated timeline for machine_unlearning at Steps/machine_unlearning/0204021048_1005508/machine_unlearning_stage_timeline.png\n",
      "Generated timeline for machine_unlearning at Steps/machine_unlearning/0204000906_980483/machine_unlearning_stage_timeline.png\n",
      "Generated timeline for machine_unlearning at Steps/machine_unlearning/0204002434_497948/machine_unlearning_stage_timeline.png\n",
      "Generated timeline for machine_unlearning at Steps/machine_unlearning/0203235437_311468/machine_unlearning_stage_timeline.png\n",
      "Generated timeline for machine_unlearning at Steps/machine_unlearning/0203205627_931197/machine_unlearning_stage_timeline.png\n",
      "Generated timeline for meta-learning at Steps/meta-learning/0211164506_234282/meta-learning_stage_timeline.png\n",
      "Generated timeline for meta-learning at Steps/meta-learning/0212001115_3514169/meta-learning_stage_timeline.png\n",
      "Generated timeline for meta-learning at Steps/meta-learning/0212223706_984095/meta-learning_stage_timeline.png\n",
      "Generated timeline for meta-learning at Steps/meta-learning/0212230702_2432398/meta-learning_stage_timeline.png\n",
      "Generated timeline for meta-learning at Steps/meta-learning/0212021042_3840295/meta-learning_stage_timeline.png\n",
      "Generated timeline for meta-learning at Steps/meta-learning/0211142216_3241844/meta-learning_stage_timeline.png\n",
      "Generated timeline for meta-learning at Steps/meta-learning/0212235435_1003034/meta-learning_stage_timeline.png\n",
      "Generated timeline for meta-learning at Steps/meta-learning/0211192713_3421734/meta-learning_stage_timeline.png\n",
      "Generated timeline for llm-merging at Steps/llm-merging/0121081654/llm-merging_stage_timeline.png\n",
      "Generated timeline for llm-merging at Steps/llm-merging/0124071448/llm-merging_stage_timeline.png\n",
      "Generated timeline for llm-merging at Steps/llm-merging/0124110854/llm-merging_stage_timeline.png\n",
      "Generated timeline for llm-merging at Steps/llm-merging/0122121253/llm-merging_stage_timeline.png\n",
      "Generated timeline for llm-merging at Steps/llm-merging/0122132158/llm-merging_stage_timeline.png\n",
      "Generated timeline for llm-merging at Steps/llm-merging/0121200118/llm-merging_stage_timeline.png\n",
      "Generated timeline for llm-merging at Steps/llm-merging/0121073211/llm-merging_stage_timeline.png\n",
      "Generated timeline for llm-merging at Steps/llm-merging/0121214229/llm-merging_stage_timeline.png\n",
      "Generated timeline for backdoor-trigger-recovery at Steps/backdoor-trigger-recovery/0124032241/backdoor-trigger-recovery_stage_timeline.png\n",
      "Generated timeline for backdoor-trigger-recovery at Steps/backdoor-trigger-recovery/0121200721/backdoor-trigger-recovery_stage_timeline.png\n",
      "Generated timeline for backdoor-trigger-recovery at Steps/backdoor-trigger-recovery/0124144709/backdoor-trigger-recovery_stage_timeline.png\n",
      "Generated timeline for backdoor-trigger-recovery at Steps/backdoor-trigger-recovery/0122092147/backdoor-trigger-recovery_stage_timeline.png\n",
      "Generated timeline for backdoor-trigger-recovery at Steps/backdoor-trigger-recovery/0121045246/backdoor-trigger-recovery_stage_timeline.png\n",
      "Generated timeline for backdoor-trigger-recovery at Steps/backdoor-trigger-recovery/0122164524/backdoor-trigger-recovery_stage_timeline.png\n",
      "Generated timeline for backdoor-trigger-recovery at Steps/backdoor-trigger-recovery/0121105812/backdoor-trigger-recovery_stage_timeline.png\n",
      "Generated timeline for backdoor-trigger-recovery at Steps/backdoor-trigger-recovery/0122014648/backdoor-trigger-recovery_stage_timeline.png\n",
      "Generated timeline for perception_temporal_action_loc at Steps/perception_temporal_action_loc/0130064726/perception_temporal_action_loc_stage_timeline.png\n",
      "Generated timeline for perception_temporal_action_loc at Steps/perception_temporal_action_loc/0130104224/perception_temporal_action_loc_stage_timeline.png\n",
      "Generated timeline for perception_temporal_action_loc at Steps/perception_temporal_action_loc/0130021740/perception_temporal_action_loc_stage_timeline.png\n",
      "Generated timeline for perception_temporal_action_loc at Steps/perception_temporal_action_loc/0130045001/perception_temporal_action_loc_stage_timeline.png\n",
      "Generated timeline for perception_temporal_action_loc at Steps/perception_temporal_action_loc/0130093111/perception_temporal_action_loc_stage_timeline.png\n",
      "Generated timeline for perception_temporal_action_loc at Steps/perception_temporal_action_loc/0130112112/perception_temporal_action_loc_stage_timeline.png\n",
      "Generated timeline for perception_temporal_action_loc at Steps/perception_temporal_action_loc/0130102918/perception_temporal_action_loc_stage_timeline.png\n",
      "Generated timeline for perception_temporal_action_loc at Steps/perception_temporal_action_loc/0130102836/perception_temporal_action_loc_stage_timeline.png\n"
     ]
    }
   ],
   "source": [
    "root_dir = 'Steps'\n",
    "\n",
    "# Find all analysis.json files\n",
    "analysis_files = find_analysis_files(root_dir)\n",
    "\n",
    "print(f\"Found {len(analysis_files)} analysis.json files to process\")\n",
    "\n",
    "# Process each file\n",
    "for file_path in analysis_files:\n",
    "    # Get the directory containing the analysis.json file\n",
    "    output_dir = os.path.dirname(file_path)\n",
    "    \n",
    "    # Process the file\n",
    "    process_analysis_json(file_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_analysis_json(file_path):\n",
    "    \"\"\"\n",
    "    Process a single analysis.json file and extract timeline data.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the analysis.json file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing timeline data\n",
    "    \"\"\"\n",
    "    # Load the JSON data\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Check if data has the expected structure\n",
    "    if 'steps' not in data:\n",
    "        print(f\"Warning: {file_path} does not contain 'steps' key, skipping\")\n",
    "        return None\n",
    "    \n",
    "    # Process the data\n",
    "    steps = data['steps']\n",
    "    \n",
    "    # Extract run ID from the file path (timestamp directory)\n",
    "    path_parts = file_path.split(os.sep)\n",
    "    run_id = path_parts[-2] if len(path_parts) >= 2 else \"Unknown_Run\"\n",
    "    \n",
    "    # Group steps by stage transitions\n",
    "    transitions = []\n",
    "    current_stage = None\n",
    "    start_step = None\n",
    "    start_time = None\n",
    "    last_step_id = None\n",
    "    last_step_time = None\n",
    "    \n",
    "    for step in steps:\n",
    "        # Skip steps without Stage information\n",
    "        if 'Stage' not in step:\n",
    "            continue\n",
    "            \n",
    "        stage = step['Stage']\n",
    "        step_time = step.get('Timestamp')\n",
    "        \n",
    "        # Keep track of the last step in the current stage\n",
    "        if stage == current_stage:\n",
    "            last_step_id = step['Step_ID']\n",
    "            last_step_time = step_time\n",
    "        \n",
    "        # When stage changes, record the previous stage block\n",
    "        elif current_stage is not None:\n",
    "            # For stages with only a single step:\n",
    "            if start_step == last_step_id:\n",
    "                # If this is not the first step in the entire process\n",
    "                prev_step_time = None\n",
    "                for prev_step in steps:\n",
    "                    if prev_step['Step_ID'] == start_step - 1:\n",
    "                        prev_step_time = prev_step.get('Timestamp')\n",
    "                        break\n",
    "                \n",
    "                # If we found the previous step, use its timestamp as the start time\n",
    "                if prev_step_time is not None and last_step_time is not None:\n",
    "                    duration = last_step_time - prev_step_time\n",
    "                else:\n",
    "                    # Fallback: estimate based on the current step\n",
    "                    duration = 5  # default seconds for a single step with no context\n",
    "            else:\n",
    "                # For multi-step stages, the duration is from the prior stage's last step\n",
    "                # to this stage's last step\n",
    "                duration = last_step_time - start_time if (start_time is not None and last_step_time is not None) else None\n",
    "            \n",
    "            # Add the previous stage block\n",
    "            transitions.append({\n",
    "                'stage': current_stage,\n",
    "                'start_step': start_step,\n",
    "                'end_step': last_step_id,\n",
    "                'position': len(transitions),\n",
    "                'start_time': start_time,\n",
    "                'end_time': last_step_time,\n",
    "                'duration': duration\n",
    "            })\n",
    "            \n",
    "            # Start a new stage block\n",
    "            current_stage = stage\n",
    "            start_step = step['Step_ID']\n",
    "            \n",
    "            # Find the timestamp of the previous step to use as start_time\n",
    "            prev_step_time = None\n",
    "            for prev_step in steps:\n",
    "                if prev_step['Step_ID'] == start_step - 1:\n",
    "                    prev_step_time = prev_step.get('Timestamp')\n",
    "                    break\n",
    "            \n",
    "            # Use previous step's timestamp as start_time\n",
    "            start_time = prev_step_time\n",
    "            \n",
    "            last_step_id = step['Step_ID']\n",
    "            last_step_time = step_time\n",
    "        \n",
    "        # Initialize the first stage\n",
    "        else:\n",
    "            current_stage = stage\n",
    "            start_step = step['Step_ID']\n",
    "            \n",
    "            # For the first step, use a reasonable time before\n",
    "            if step_time is not None:\n",
    "                # Estimate start time as 5 seconds before first timestamp\n",
    "                start_time = step_time - 5  # assuming the first step took 5 seconds\n",
    "            else:\n",
    "                start_time = None\n",
    "                \n",
    "            last_step_id = step['Step_ID']\n",
    "            last_step_time = step_time\n",
    "    \n",
    "    # Add the final stage block\n",
    "    if current_stage is not None:\n",
    "        # For single step final stage\n",
    "        if start_step == last_step_id:\n",
    "            # Try to find the previous step timestamp\n",
    "            prev_step_time = None\n",
    "            for prev_step in steps:\n",
    "                if prev_step['Step_ID'] == start_step - 1:\n",
    "                    prev_step_time = prev_step.get('Timestamp')\n",
    "                    break\n",
    "            \n",
    "            # Calculate duration from previous step to current\n",
    "            if prev_step_time is not None and last_step_time is not None:\n",
    "                duration = last_step_time - prev_step_time\n",
    "            else:\n",
    "                # If no previous step, estimate a reasonable duration\n",
    "                duration = 5  # default seconds\n",
    "        else:\n",
    "            # For multi-step stages\n",
    "            duration = last_step_time - start_time if (start_time is not None and last_step_time is not None) else None\n",
    "        \n",
    "        transitions.append({\n",
    "            'stage': current_stage,\n",
    "            'start_step': start_step,\n",
    "            'end_step': last_step_id,\n",
    "            'position': len(transitions),\n",
    "            'start_time': start_time,\n",
    "            'end_time': last_step_time,\n",
    "            'duration': duration\n",
    "        })\n",
    "    \n",
    "    # Count steps in each stage for proportional sizing\n",
    "    for t in transitions:\n",
    "        t['step_count'] = t['end_step'] - t['start_step'] + 1\n",
    "    \n",
    "    # Calculate total duration\n",
    "    total_duration = sum(t['duration'] for t in transitions if t['duration'] is not None)\n",
    "    \n",
    "    return {\n",
    "        'run_id': run_id,\n",
    "        'transitions': transitions,\n",
    "        'total_duration': total_duration\n",
    "    }\n",
    "\n",
    "def draw_timeline(ax, timeline_data, y_pos, height, stage_colors, stage_names, run_id, width_scale):\n",
    "    \"\"\"\n",
    "    Draw a single timeline on the given axes\n",
    "    \n",
    "    Args:\n",
    "        ax: Matplotlib axes to draw on\n",
    "        timeline_data: Dictionary containing timeline data\n",
    "        y_pos: Y position for this timeline\n",
    "        height: Height of the timeline\n",
    "        stage_colors: Dictionary mapping stage numbers to colors\n",
    "        stage_names: Dictionary mapping stage numbers to names\n",
    "        run_id: ID of this run (for labeling)\n",
    "        width_scale: Factor to scale duration to figure width (seconds per unit width)\n",
    "    \"\"\"\n",
    "    transitions = timeline_data['transitions']\n",
    "    total_width = 1.0  # Total width of the figure is 1.0\n",
    "    \n",
    "    # Add run label on the left with more space\n",
    "    ax.text(-0.05, y_pos, run_id, ha='right', va='center', fontsize=9, fontfamily='monospace')\n",
    "    \n",
    "    # Draw timeline background - full width rectangle with light gray\n",
    "    rect_bg = patches.Rectangle(\n",
    "        (0, y_pos - height/2),\n",
    "        total_width,\n",
    "        height,\n",
    "        linewidth=0,\n",
    "        facecolor='#f8f8f8',\n",
    "        alpha=0.5\n",
    "    )\n",
    "    ax.add_patch(rect_bg)\n",
    "    \n",
    "    # Calculate total duration for scaling\n",
    "    total_duration = timeline_data['total_duration']\n",
    "    if total_duration <= 0:\n",
    "        print(f\"Warning: Run {run_id} has no duration information\")\n",
    "        return\n",
    "    \n",
    "    # The blocks will be drawn with width proportional to their duration using the common scale\n",
    "    current_pos = 0\n",
    "    \n",
    "    # Draw the timeline blocks\n",
    "    for i, block in enumerate(transitions):\n",
    "        # Skip blocks with no duration\n",
    "        if block['duration'] is None or block['duration'] <= 0:\n",
    "            continue\n",
    "            \n",
    "        # Calculate width using the common scale for all timelines\n",
    "        # This ensures 1 hour on one timeline looks the same as 1 hour on another\n",
    "        block_width = block['duration'] / width_scale\n",
    "        \n",
    "        # Ensure minimum visibility\n",
    "        min_width = 0.0035\n",
    "        if block_width < min_width:\n",
    "            block_width = min_width\n",
    "        \n",
    "        # Create rectangle\n",
    "        rect = patches.Rectangle(\n",
    "            (current_pos, y_pos - height/2),\n",
    "            block_width,\n",
    "            height,\n",
    "            linewidth=0.3,\n",
    "            edgecolor='black',\n",
    "            facecolor=stage_colors.get(block['stage'], \"#cccccc\"),  # Default to gray if stage not in colors\n",
    "            alpha=0.8\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add step count to each block - always try to show the count\n",
    "        mid_point = current_pos + block_width/2\n",
    "        \n",
    "        # For narrow blocks, show number but smaller\n",
    "        font_size = 7\n",
    "        if block_width < 0.05:\n",
    "            font_size = 6\n",
    "        \n",
    "        # Always show the step count\n",
    "        # ax.text(\n",
    "        #     mid_point, y_pos, \n",
    "        #     f\"{block['step_count']}\", \n",
    "        #     ha='center', va='center', \n",
    "        #     fontsize=font_size, \n",
    "        #     fontweight='bold',\n",
    "        #     color='black'\n",
    "        # )\n",
    "        \n",
    "        # Move to next position\n",
    "        current_pos += block_width\n",
    "    \n",
    "    # Add total duration at the end of timeline\n",
    "    if total_duration is not None:\n",
    "        # Format duration\n",
    "        if total_duration >= 3600:\n",
    "            hours = int(total_duration // 3600)\n",
    "            minutes = int((total_duration % 3600) // 60)\n",
    "            duration_str = f\"{hours}h {minutes}m\"\n",
    "        elif total_duration >= 60:\n",
    "            minutes = int(total_duration // 60)\n",
    "            seconds = int(total_duration % 60)\n",
    "            duration_str = f\"{minutes}m {seconds}s\"\n",
    "        else:\n",
    "            duration_str = f\"{int(total_duration)}s\"\n",
    "        \n",
    "        ax.text(1.05, y_pos, duration_str, ha='left', va='center', fontsize=8, fontfamily='monospace')\n",
    "\n",
    "def create_task_timeline(task_name, run_data, output_dir):\n",
    "    \"\"\"\n",
    "    Create a timeline visualization for all runs of a task\n",
    "    \n",
    "    Args:\n",
    "        task_name: Name of the task\n",
    "        run_data: List of dictionaries containing timeline data for each run\n",
    "        output_dir: Directory to save the output figure\n",
    "    \"\"\"\n",
    "    # Define stage names\n",
    "    stage_names = {\n",
    "        1: \"understand\",\n",
    "        2: \"implement\",\n",
    "        3: \"execute\",\n",
    "        4: \"improve\",\n",
    "        5: \"submit\"\n",
    "    }\n",
    "    \n",
    "    # Define stage colors\n",
    "    stage_colors = {\n",
    "        1: \"#b3e6b3\",  # Light green\n",
    "        2: \"#b3b3e6\",  # Light blue\n",
    "        3: \"#e6b3b3\",  # Light red\n",
    "        4: \"#e6e6b3\",  # Light yellow\n",
    "        5: \"#e6b3e6\"   # Light purple\n",
    "    }\n",
    "    \n",
    "    # Sort runs by run_id\n",
    "    sorted_runs = sorted(run_data, key=lambda x: x['run_id'])\n",
    "    \n",
    "    # Calculate a common width scale for all timelines\n",
    "    # Find max duration across all runs\n",
    "    max_duration = max([r['total_duration'] for r in sorted_runs if r['total_duration'] > 0], default=3600)\n",
    "    \n",
    "    # We want to scale so that the timeline with maximum duration fills 90% of the width\n",
    "    # This gives some room for labels\n",
    "    width_scale = max_duration / 0.9  # seconds per unit width\n",
    "    \n",
    "    # Create figure with appropriate dimensions\n",
    "    num_runs = len(sorted_runs)\n",
    "    fig_width = 15  # Wider figure for better readability\n",
    "    fig_height = max(7, 1.5 + 0.3 * num_runs)  # Adjust height based on number of runs\n",
    "    fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "    \n",
    "    # Set up the figure with a white background\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    # Calculate spacing for timelines\n",
    "    timeline_height = 0.1  # Height of each timeline\n",
    "    y_spacing = 1.0 / (num_runs + 1)  # Equal spacing based on number of runs\n",
    "    \n",
    "    # Draw vertical gridlines first (every 10% of width)\n",
    "    for x in np.arange(0, 1.1, 0.1):\n",
    "        ax.axvline(x=x, color='lightgray', linestyle='-', linewidth=0.5, alpha=0.7)\n",
    "    \n",
    "    # Draw horizontal gridlines - one per run (top and bottom of each row)\n",
    "    for i in range(num_runs):\n",
    "        y = 1.0 - y_spacing * (i + 1)\n",
    "        ax.axhline(y=y - timeline_height/2, color='lightgray', linestyle='-', linewidth=0.5, alpha=0.7)\n",
    "        ax.axhline(y=y + timeline_height/2, color='lightgray', linestyle='-', linewidth=0.5, alpha=0.7)\n",
    "    \n",
    "    # Draw each timeline using the common scale\n",
    "    for i, run_data in enumerate(sorted_runs):\n",
    "        y_pos = 1.0 - y_spacing * (i + 1)\n",
    "        draw_timeline(ax, run_data, y_pos, timeline_height, stage_colors, stage_names, run_data['run_id'], width_scale)\n",
    "    \n",
    "    # Format task name for display\n",
    "    display_task_name = task_name.replace('_', ' ').title()\n",
    "    \n",
    "    # Add title\n",
    "    plt.title(f'{display_task_name} - Stage Timelines Across Runs', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Add a legend for stages\n",
    "    legend_elements = []\n",
    "    for stage in sorted(stage_names.keys()):\n",
    "        legend_elements.append(\n",
    "            patches.Patch(facecolor=stage_colors[stage], \n",
    "                          edgecolor='black', \n",
    "                          label=f'Stage {stage}: {stage_names[stage]}')\n",
    "        )\n",
    "    \n",
    "    # Position legend at the bottom with enough space\n",
    "    legend = plt.legend(handles=legend_elements, loc='upper center', \n",
    "               bbox_to_anchor=(0.5, -0.05 - 0.01 * num_runs), ncol=5,\n",
    "               fancybox=True, shadow=True)\n",
    "    legend.get_frame().set_linewidth(0.5)\n",
    "    \n",
    "    # Add note about timeline visualization (positioned based on number of runs)\n",
    "    plt.figtext(0.5, 0.01, \n",
    "                \"Note: Block widths are proportional to time duration in each stage.\\nNumbers inside blocks show step count. Total duration shown on the right.\", \n",
    "                ha='center', fontsize=9, \n",
    "                bbox=dict(facecolor='white', alpha=0.8, edgecolor='lightgray', boxstyle='round,pad=0.5'))\n",
    "    \n",
    "    # Set up the axes\n",
    "    ax.set_xlim(-0.15, 1.15)  # More space on sides for labels\n",
    "    ax.set_ylim(0, 1.2)       # More space on top\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Add border around the plot\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(1.5)\n",
    "        spine.set_edgecolor('black')\n",
    "    \n",
    "    # Add more space at the bottom for legend and notes\n",
    "    plt.subplots_adjust(bottom=0.15 + 0.02 * num_runs, left=0.15, right=0.85, top=0.95)\n",
    "    \n",
    "    # Generate output filename\n",
    "    output_filename = os.path.join(output_dir, f\"{task_name}_all_runs_timeline.png\")\n",
    "    \n",
    "    # Save the figure without using tight_layout (which causes warnings)\n",
    "    plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f\"Generated timeline for {task_name} at {output_filename}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def find_analysis_files(root_dir):\n",
    "    \"\"\"\n",
    "    Find all analysis.json files under the root directory\n",
    "    \n",
    "    Args:\n",
    "        root_dir: Root directory to start the search\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping task names to lists of file paths\n",
    "    \"\"\"\n",
    "    task_files = defaultdict(list)\n",
    "    \n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        if 'analysis.json' in filenames:\n",
    "            file_path = os.path.join(dirpath, 'analysis.json')\n",
    "            # Extract task name (Steps/task_name/run_id/analysis.json)\n",
    "            path_parts = file_path.split(os.sep)\n",
    "            if len(path_parts) >= 3 and path_parts[0] == root_dir:\n",
    "                task_name = path_parts[1]\n",
    "                task_files[task_name].append(file_path)\n",
    "    \n",
    "    return task_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 analysis.json files across 5 tasks\n",
      "Processing task: machine_unlearning with 8 runs\n",
      "Generated timeline for machine_unlearning at Steps/machine_unlearning/machine_unlearning_all_runs_timeline.png\n",
      "Processing task: meta-learning with 8 runs\n",
      "Generated timeline for meta-learning at Steps/meta-learning/meta-learning_all_runs_timeline.png\n",
      "Processing task: llm-merging with 8 runs\n",
      "Generated timeline for llm-merging at Steps/llm-merging/llm-merging_all_runs_timeline.png\n",
      "Processing task: backdoor-trigger-recovery with 8 runs\n",
      "Generated timeline for backdoor-trigger-recovery at Steps/backdoor-trigger-recovery/backdoor-trigger-recovery_all_runs_timeline.png\n",
      "Processing task: perception_temporal_action_loc with 8 runs\n",
      "Generated timeline for perception_temporal_action_loc at Steps/perception_temporal_action_loc/perception_temporal_action_loc_all_runs_timeline.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Root directory to search for analysis.json files\n",
    "root_dir = 'Steps'\n",
    "\n",
    "# Find all analysis.json files grouped by task\n",
    "task_files = find_analysis_files(root_dir)\n",
    "\n",
    "print(f\"Found {sum(len(files) for files in task_files.values())} analysis.json files across {len(task_files)} tasks\")\n",
    "\n",
    "# Process each task\n",
    "for task_name, file_paths in task_files.items():\n",
    "    print(f\"Processing task: {task_name} with {len(file_paths)} runs\")\n",
    "    \n",
    "    # Process each run for this task\n",
    "    run_data = []\n",
    "    for file_path in file_paths:\n",
    "        timeline_data = process_analysis_json(file_path)\n",
    "        if timeline_data:\n",
    "            run_data.append(timeline_data)\n",
    "    \n",
    "    # Generate a consolidated timeline for this task's runs\n",
    "    if run_data:\n",
    "        output_dir = os.path.join(root_dir, task_name)\n",
    "        create_task_timeline(task_name, run_data, output_dir)\n",
    "    else:\n",
    "        print(f\"No valid data found for task: {task_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
